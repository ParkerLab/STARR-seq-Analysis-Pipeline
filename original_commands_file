#!/usr/bin/env python3
# -*- coding: utf-8 -*-

#
# This analysis was generated with this mka command:
#
# /lab/sw/ve/pl3/bin/mka --analysis-type 'atac-seq' --description 'test' /lab/work/collinwa/scifair_1718/run_1890_pipeline_test /lab/work/collinwa/scifair_1718/data_2/________1___Rep1.1.fastq.gz /lab/work/collinwa/scifair_1718/data_2/________1___Rep1.2.fastq.gz /lab/work/collinwa/scifair_1718/data_2/________2___Rep1.1.fastq.gz /lab/work/collinwa/scifair_1718/data_2/________2___Rep1.2.fastq.gz
#
# run in this directory:
#
# /lab/work/collinwa/scifair_1718
#

from __future__ import print_function

import contextlib
import gzip
import functools
import itertools
import os
import re
import sys


REFERENCE_ROOT = os.getenv('MKA_REFERENCE_ROOT', '/lab/data/reference')

prefix_reference_root = functools.partial(os.path.join, REFERENCE_ROOT)

BWA_REFERENCES = {
    'dm3': prefix_reference_root('fly/dm3/index/bwa/0.7.12/dm3'),
    'dm6': prefix_reference_root('fly/dm6/index/bwa/0.7.12/dm6'),
    'hg19': prefix_reference_root('human/hg19/index/bwa/0.7.12/hg19'),
    'mm9': prefix_reference_root('mouse/mm9/index/bwa/0.7.12/mm9'),
    'mm10': prefix_reference_root('mouse/mm10/index/bwa/0.7.12/mm10'),
    'rn5': prefix_reference_root('rat/rn5/index/bwa/0.7.12/rn5'),
}

AUTOSOMAL_REFERENCES = {
    'dm3': ['chr2L', 'chr2LHet', 'chr2R', 'chr2RHet', 'chr3L', 'chr3LHet', 'chr3R', 'chr3RHet', 'chr4'],
    'dm6': ['chr2L', 'chr2R', 'chr3L', 'chr3R', 'chr4'],
    'hg19': ['chr{}'.format(i) for i in range(1, 23)],
    'mm9': ['chr{}'.format(i) for i in range(1, 20)],
    'mm10': ['chr{}'.format(i) for i in range(1, 20)],
    'rn5': ['chr{}'.format(i) for i in range(1, 21)],
}

EXCLUDED_REGIONS = {
    'dm3': list(map(prefix_reference_root, ['fly/dm3/annot/dm3-blacklist.bed.gz'])),
    'hg19': list(map(prefix_reference_root, ['human/hg19/annot/wgEncodeDukeMapabilityRegionsExcludable.bed.gz', 'human/hg19/annot/wgEncodeDacMapabilityConsensusExcludable.bed.gz'])),
    'mm9': list(map(prefix_reference_root, ['mouse/mm9/annot/mm9-blacklist.bed.gz'])),
    'mm10': list(map(prefix_reference_root, ['mouse/mm10/annot/mm10.blacklist.bed.gz']))
}

TSS_REFERENCES = {
    'hg19': prefix_reference_root('human/hg19/annot/hg19.tss.refseq.bed'),
    'mm9': prefix_reference_root('mouse/mm9/annot/mm9.tss.refseq.bed'),
    'mm10': prefix_reference_root('mouse/mm10/annot/mm10.tss.refseq.bed'),
    'rn5': prefix_reference_root('rat/rn5/annot/rn5.tss.refseq.bed'),
}

FASTQ_RE = re.compile('\.f(ast)?q(\.gz)?$')

MACS2_GENOME_SIZES = {
    'dm3': 'dm',
    'hg19': 'hs',
    'mm9': 'mm',
    'mm10': 'mm',
    'rn5': 'mm'
}

ORGANISMS = {
    'dm3': 'fly',
    'dm6': 'fly',
    'hg19': 'human',
    'mm9': 'mouse',
    'mm10': 'mouse',
    'rn5': 'rat'
}

ANALYSIS_NAME = "run_1890_pipeline_test"
DESCRIPTION = """test"""
CONTROL_PATH = "/lab/work/collinwa/scifair_1718/run_1890_pipeline_test"
ANALYSIS_PATH = "/lab/work/collinwa/scifair_1718/run_1890_pipeline_test"
DATA_PATH = os.path.join(ANALYSIS_PATH, 'data')
WORK_PATH = os.path.join(ANALYSIS_PATH, 'work')
PIPELINE = os.path.join(ANALYSIS_PATH, 'pipeline')

ATAQV_DIR = os.path.join(WORK_PATH, 'ataqv')
BWA_DIR = os.path.join(WORK_PATH, 'bwa')
FASTQC_DIR = os.path.join(WORK_PATH, 'fastqc')
MACS2_DIR = os.path.join(WORK_PATH, 'macs2')
MD_DIR = os.path.join(WORK_PATH, 'mark_duplicates')
PRUNE_DIR = os.path.join(WORK_PATH, 'prune')
TRIM_ADAPTER_DIR = os.path.join(WORK_PATH, 'trim_adapters')
GB_DIR = os.path.join(WORK_PATH, 'gb')

# By default, we use ionice and limit the number of particularly
# I/O-intensive jobs that run at once, to keep the machine
# responsive. If you're running on dedicated cluster nodes, you
# probably want to set this to 0.
LIMIT_IO = 0

#
# Library dictionary
#

LIBRARIES = {'': {'sequencing_date': '', 'reference_genome': 'hg19', 'sequencing_center': '', 'sample': '', 'analysis_specific_options': {}, 'sequencing_platform': '', 'sequencing_platform_model': '', 'url': '', 'readgroups': {'__2': ['/lab/work/collinwa/scifair_1718/data_2/________2___Rep1.1.fastq.gz', '/lab/work/collinwa/scifair_1718/data_2/________2___Rep1.2.fastq.gz'], '__1': ['/lab/work/collinwa/scifair_1718/data_2/________1___Rep1.1.fastq.gz', '/lab/work/collinwa/scifair_1718/data_2/________1___Rep1.2.fastq.gz']}, 'library': '', 'description': 'Rep1'}}

SAMPLES = {}
for library in LIBRARIES.values():
    SAMPLES.setdefault(library['sample'], []).append(library)


def maybe_gzip(filename, ioniced=False):
    """Compress a file with gzip."""
    template_data = {
        'f': filename,
        'ionice': ioniced and 'ionice -c 2 -n 7 ' or ''
    }

    command_template = """if [ -r "{f}" ]; then {ionice}gzip -f "{f}"; elif [ -r "{f}".gz ]; then echo '"{f}" already gzipped.'; fi"""

    printp(command_template.format(**template_data))


def mkdir(dir, mode=0o0750):
    """Construct a directory hierarchy using the given permissions."""
    if not os.path.exists(dir):
        os.makedirs(dir, mode)


def open_maybe_gzipped(filename):
    """
    Open a possibly gzipped file.

    Parameters
    ----------
    filename: str
        The name of the file to open.

    Returns
    -------
    file
        An open file object.
    """
    with open(filename, 'rb') as test_read:
        byte1, byte2 = test_read.read(1), test_read.read(1)
        if byte1 and ord(byte1) == 0x1f and byte2 and ord(byte2) == 0x8b:
            f = gzip.open(filename, mode='rt')
        else:
            f = open(filename, 'rt')
    return f


LEADING_WHITESPACE_RE = re.compile(r'(\s+)*(.*)')
def print_to_pipeline(pipeline_file, text=None, timed=False, ioniced=False):
    """The primary function of all this: writing to a drmr script."""
    if text:
        m = LEADING_WHITESPACE_RE.match(text)
        if m.group(1):
            pipeline_file.write(m.group(1))
        if timed:
            pipeline_file.write('/usr/bin/time -v ')
        if ioniced:
            pipeline_file.write('ionice -c 2 -n 7 ')
        pipeline_file.write(m.group(2))
        pipeline_file.write('\n')


@contextlib.contextmanager
def working_directory(path):
    """Changes to the given directory, returning to the original working directory when the context block is exited."""
    original_directory = os.getcwd()
    try:
        os.chdir(path)
        yield
    finally:
        os.chdir(original_directory)


def symlink(source_path, dest_path, absolute=False):
    """Create a symbolic link from the source_path to the dest_path, which can be a directory."""

    workdir = os.path.isdir(dest_path) and dest_path or os.path.dirname(dest_path)

    with working_directory(workdir):
        src = os.path.normpath(absolute and os.path.abspath(source_path) or os.path.relpath(source_path, dest_path))
        dest = dest_path
        dest_base = os.path.basename(dest)
        if os.path.isdir(dest_path):
            dest = os.path.join(dest_path, os.path.basename(src))
            if os.path.lexists(dest):
                os.unlink(dest)
            os.symlink(src, dest)
        else:
            mkdir(os.path.dirname(dest_path))
            if os.path.lexists(dest):
                os.unlink(dest)
            os.symlink(src, dest)
        return dest, dest_base


def rename_fastq(fastq, suffix=''):
    return FASTQ_RE.sub(fastq, suffix)


def iterate_library_source_files(library_name):
    """Generates a list of the library's original files."""
    library = LIBRARIES[library_name]
    for rg, files in sorted(library['readgroups'].items()):
        for f in sorted(files):
            yield f


def iterate_all_source_files():
    return itertools.chain(*[iterate_library_source_files(library_name) for library_name in sorted(LIBRARIES.keys())])


def iterate_library_files(library_name):
    """Generates a list of the library's files in DATA_PATH."""
    library = LIBRARIES[library_name]
    for rg, files in sorted(library['readgroups'].items()):
        for f in sorted(files):
            yield os.path.join(DATA_PATH, os.path.basename(f))


def iterate_all_files():
    return itertools.chain(*[iterate_library_files(library_name) for library_name in sorted(LIBRARIES.keys())])


def library_reference_genomes():
    return sorted(list(set(library['reference_genome'] for library_name, library in sorted(LIBRARIES.items()))))


def libraries_by_genome():
    libraries = {}
    for genome in library_reference_genomes():
        libraries[genome] = [library for library_name, library in sorted(LIBRARIES.items()) if library['reference_genome'] == genome]

    # return genome, libraries for each genom
    return sorted(libraries.items())


def get_chromosome_sizes_path(library):
    reference_genome = library['reference_genome']
    bwa_reference = BWA_REFERENCES[reference_genome]
    return '{}.chrom_sizes'.format(bwa_reference)


def make_read_group_file(library_name, readgroup, suffix=''):
    return '{library_name}___{readgroup}{suffix}'.format(**locals())


def make_read_group_header(library, id):
    read_group_components = {
        'ID': '{}___{}'.format(library['library'], id),

        # library
        'LB': library['library'],

        # sample
        'SM': library['sample'],

        # sequencing center name
        'CN': library['sequencing_center'],

        # ISO8601 date(time) of sequencing
        'DT': library['sequencing_date'],

        # platform (Illumina, Solid, etc. -- see the spec for valid values
        'PL': library['sequencing_platform'],

        # free-form description
        'DS': library['description'].replace('\n', ' '),
    }

    header = """@RG\\t{}""".format('\\t'.join('{}:{}'.format(k, v) for k, v in sorted(read_group_components.items()) if v))

    return header


def fastqc():
    """Run FastQC on all input libraries."""

    mkdir(FASTQC_DIR)

    printp("""\n#\n# run FastQC on initial data\n#""")
    printp("""\n# drmr:label fastqc""")
    printp("""\n# drmr:job time_limit=2h working_directory={}""".format(FASTQC_DIR))

    for index, f in enumerate(iterate_all_files(), 1):
        symlink(f, FASTQC_DIR)
        printp("""fastqc {}""".format(os.path.basename(f)), timed=True, ioniced=True)
        if LIMIT_IO and index % LIMIT_IO == 0:
            # limit the number of concurrent jobs to avoid thrashing the disk (set LIMIT_IO=False on clusters!)
            printp("""\n# drmr:wait""")


def trim_adapters():
    #
    # Trim adapter sequence from the FASTQ files
    #

    mkdir(TRIM_ADAPTER_DIR)

    printp("""\n#\n# trim adapter sequence from reads\n#""")
    printp("""\n# drmr:label trim-adapters""")
    printp("""\n# drmr:job time_limit=4h working_directory={}""".format(TRIM_ADAPTER_DIR))

    index = 0
    for name, library in sorted(LIBRARIES.items()):
        for rg, files in sorted(library['readgroups'].items()):
            for f in sorted(files):
                symlink(os.path.join(DATA_PATH, os.path.basename(f)), TRIM_ADAPTER_DIR)
            bases = ' '.join(os.path.basename(f) for f in sorted(files))
            printp("""cta {}""".format(bases), timed=True, ioniced=True)
            index += 1

            if LIMIT_IO and index % LIMIT_IO == 0:
                # limit the number of concurrent jobs to avoid thrashing the disk (set LIMIT_IO=False on clusters!)
                printp("""\n# drmr:wait""")


def bwa(threads=4, algorithm='MEM', time_limit='8h'):
    """
    Aligns reads to the reference genome with BWA.

    The BWA algorithm can be specified as 'MEM', 'backtrack', or
    'auto', to choose the algorithm based on the library read size.
    """

    mkdir(BWA_DIR)

    printp("""# drmr:label bwa\n""")
    printp("""# drmr:job nodes=1 processors={} working_directory={} time_limit={}""".format(threads, BWA_DIR, time_limit))

    backtrack_readgroups = []
    for library_name, library in sorted(LIBRARIES.items()):
        reference_genome = library['reference_genome']
        bwa_reference = BWA_REFERENCES[reference_genome]
        if not (os.path.exists(bwa_reference) or os.path.exists(bwa_reference + '.bwt')):
            print('BWA reference "{}" does not exist.'.format(bwa_reference), file=sys.stderr)
            sys.exit(1)

        for rg, files in sorted(library['readgroups'].items()):
            fastq_files = sorted([rename_fastq('.trimmed.fq.gz', os.path.basename(f)) for f in files])
            for f in fastq_files:
                symlink(os.path.join(TRIM_ADAPTER_DIR, f), BWA_DIR)

            if algorithm == 'auto':
                algorithm = 'MEM'
                try:
                    with open_maybe_gzipped(files[0]) as f:
                        next(f)
                        seq = next(f)
                        read_length = len(seq)
                        if read_length < 70:
                            algorithm = 'backtrack'
                    printp("""# Library {} read group {} read length: {}""".format(library_name, rg, read_length))
                except Exception as e:
                    printp("""# Could not determine read length for library {} read group {}: {}""".format(library_name, rg, e))

                printp("""# Using bwa's {} algorithm.""".format(algorithm))

            printp("""\n#\n# align reads in {library_name} read group {rg} to {reference_genome} with bwa {algorithm}\n#\n""".format(**locals()))

            if algorithm == 'MEM':
                bwa_input_files = ' '.join(fastq_files)
                read_group_header = make_read_group_header(library, rg)
                bam = make_read_group_file(library_name, rg, '.bam')
                printp("""bwa mem -M -R '{read_group_header}' -t {threads} {bwa_reference} {bwa_input_files} | samtools sort -m 1g -@ {threads} -O bam -T {library_name}___{rg}.sort -o {bam} -""".format(**locals()), timed=True, ioniced=True)

            elif algorithm == 'backtrack':
                backtrack_readgroups.append((library, rg, fastq_files))
                for input_file in fastq_files:
                    output_file = input_file.replace('.trimmed.fq.gz', '.sai')
                    printp("""bwa aln -t {threads} -f {output_file} {bwa_reference} {input_file}""".format(**locals()), timed=True, ioniced=True)

    printp("""\n# drmr:wait""")

    if backtrack_readgroups:
        printp("""\n#\n# Create the BAM files.\n#\n""")
        printp("""# drmr:job nodes=1 processors={} working_directory={} time_limit={}""".format(threads, BWA_DIR, time_limit))

    for library, rg, fastq_files in backtrack_readgroups:
        if len(fastq_files) == 1:
            bwa_command = 'samse'
        elif len(fastq_files) == 2:
            bwa_command = 'sampe'
        else:
            print('Too many FASTQ files for library {}'.format(library['library']), file=sys.stderr)
            sys.exit(1)

        fastq = ' '.join(fastq_files)
        sai = ' '.join([f.replace('.trimmed.fq.gz', '.sai') for f in fastq_files])
        read_group_header = make_read_group_header(library, rg)
        bam = make_read_group_file(library['library'], rg, '.bam')
        printp("""bwa {bwa_command} -r '{read_group_header}' {bwa_reference} {sai} {fastq} | samtools sort -m 1g -@ {threads} -O bam -T {library_name}___{rg}.sort -o {bam}""".format(**locals()), timed=True, ioniced=True)

        printp("""\n# drmr:wait""")

        printp("""\n#\n# Delete output files which are not needed after bwa has run\n#""")
        printp("""\n# drmr:label clean-after-bwa""")

        printp("""\n# drmr:job time_limit=00:15:00 working_directory={}""".format(BWA_DIR))
        printp("""rm -f {}/*.sai""".format(BWA_DIR))


def mark_duplicates():
    """
    Mark duplicates in each library BAM file.
    """

    mkdir(MD_DIR)

    printp("""# drmr:label mark-duplicates\n""")
    printp("""# drmr:job nodes=1 processors=2 memory=8g working_directory={} time_limit=8h""".format(MD_DIR))

    for library_name, library in sorted(LIBRARIES.items()):
        for rg in sorted(library['readgroups'].keys()):
            input_bam = make_read_group_file(library_name, rg, '.bam')
            output_bam = make_read_group_file(library_name, rg, '.md.bam')
            symlink(os.path.join(BWA_DIR, input_bam), MD_DIR)
            printp("""picard -m 4g MarkDuplicates I={input_bam} O={output_bam} ASSUME_SORTED=true METRICS_FILE={library_name}___{rg}.markdup.metrics VALIDATION_STRINGENCY=LENIENT TMP_DIR=.""".format(**locals()), timed=True, ioniced=True)

    printp("""\n# drmr:wait""")
    printp("""\n#\n# index the BAM files with marked duplicates, for pruning\n#\n""")
    printp("""\n# drmr:label index-md-bams\n""")

    for library_name, library in sorted(LIBRARIES.items()):
        for rg in sorted(library['readgroups'].keys()):
            bam = make_read_group_file(library_name, rg, '.md.bam')
            printp("""samtools index {bam}""".format(**locals()), timed=True, ioniced=True)


def prune(mapq=30):
    """
    Prune the BAM files down to properly paired and uniquely mapped
    autosomal alignments with good quality, and remove all duplicates
    """

    mkdir(PRUNE_DIR)

    printp("""\n# drmr:label prune\n""")
    printp("""# drmr:job nodes=1 processors=1 memory=4g time_limit=4h working_directory={}""".format(PRUNE_DIR))

    for library_name, library in sorted(LIBRARIES.items()):
        for rg in sorted(library['readgroups'].keys()):
            symlink(os.path.join(MD_DIR, make_read_group_file(library_name, rg, '.md.bam')), PRUNE_DIR)
            symlink(os.path.join(MD_DIR, make_read_group_file(library_name, rg, '.md.bam.bai')), PRUNE_DIR)

    #
    # samtools filters:
    #   -f 3: keep properly paired and mapped reads
    #   -F 4: filter out unmapped reads
    #   -F 8: filter out unmapped mates
    #   -F 256: filter out secondary reads
    #   -F 1024: filter out duplicates marked by Picard above
    #   -F 2048: filter out supplementary reads
    #
    printp("""\n#\n# prune the BAM files with marked duplicates down to properly paired""")
    printp("""# and mapped primary autosomal alignments of good quality, for peak calling\n#\n""")

    PRUNE_TEMPLATE = """samtools view -b -h -f 3 -F 4 -F 8 -F 256 -F 1024 -F 2048 -q {mapq} {input_bam} {autosomes} > {output_bam}"""

    for library_name, library in sorted(LIBRARIES.items()):
        autosomes = ' '.join(AUTOSOMAL_REFERENCES[library['reference_genome']])
        for rg in sorted(library['readgroups'].keys()):
            input_bam = make_read_group_file(library_name, rg, '.md.bam')
            output_bam = make_read_group_file(library_name, rg, '.pruned.bam')
            printp(PRUNE_TEMPLATE.format(**locals()), timed=True, ioniced=True)


def compress_macs2_output(prefix, ioniced=True):
    maybe_gzip("{}.broad_peaks.broadPeak".format(prefix), ioniced)
    maybe_gzip("{}.broad_peaks.gappedPeak".format(prefix), ioniced)
    maybe_gzip("{}.broad_control_lambda.bdg".format(prefix), ioniced)
    maybe_gzip("{}.broad_treat_pileup.bdg".format(prefix), ioniced)


def exclude_blacklisted_peaks(prefix, reference_genome):
    if reference_genome in EXCLUDED_REGIONS:
        excluded_region_files = []
        for excluded_region_file in EXCLUDED_REGIONS[reference_genome]:
            if os.path.isfile(excluded_region_file):
                excluded_region_files.append(excluded_region_file)
            else:
                print("""Excluded region file {} for reference genome {} does not exist. It cannot be used to filter MACS2 output.""".format(excluded_region_file, reference_genome), file=sys.stderr)

        exclude_commands = ' | '.join(['intersectBed -a stdin -b {} -v '.format(erf) for erf in excluded_region_files])
        if exclude_commands:
            printp("""zcat {prefix}.broad_peaks.broadPeak.gz | {exclude_commands} | gzip -c > {prefix}.broad_peaks.broadPeak.noblacklist.gz""".format(**locals()))


def macs2(shift=-100, extsize=200, exclude_blacklisted_regions=False, call_libraries=False, call_samples=False):
    """
    Call peaks with MACS2.
    """

    mkdir(MACS2_DIR)

    printp("""\n#\n# peak calling\n#""")

    printp("""\n#\n# call broad peaks (keeping dups in case of merged files)\n#\n""")
    printp("""\n# drmr:job nodes=1 processors=1 memory=8g working_directory={} time_limit=4h""".format(MACS2_DIR))

    MACS2_BROAD_PEAK_TEMPLATE = """macs2 callpeak -t {{input_bam}} -f BAM -n {{prefix}}.broad -g {{macs2_genome_size}} --nomodel --shift {shift} --extsize {extsize} -B --broad --keep-dup all > {{prefix}}.macs2.out 2>&1""".format(**locals())

    printp("""# drmr:label macs2-readgroups\n""")
    for library_name, library in sorted(LIBRARIES.items()):
        macs2_genome_size = MACS2_GENOME_SIZES[library['reference_genome']]
        for rg in sorted(library['readgroups'].keys()):
            input_bam = make_read_group_file(library_name, rg, '.pruned.bam')
            prefix = make_read_group_file(library_name, rg)
            symlink(os.path.join(PRUNE_DIR, input_bam), MACS2_DIR)
            printp(MACS2_BROAD_PEAK_TEMPLATE.format(**locals()), timed=True, ioniced=True)

    # If asked to call peaks on libraries, make a pruned BAM for each
    # then run MACS2 on those.
    if call_libraries:
        printp("""# drmr:job nodes=1 processors=1 memory=4g time_limit=4h working_directory={}""".format(PRUNE_DIR))
        printp("""\n# drmr:label make-pruned-libraries\n""")
        for index, (library_name, library) in enumerate(sorted(LIBRARIES.items()), 1):
            if LIMIT_IO and index % LIMIT_IO == 0:
                # limit the number of concurrent jobs to avoid thrashing the disk (set LIMIT_IO=False on clusters!)
                printp("""\n# drmr:wait""")

            printp("""\nsamtools merge -f {}.library.pruned.bam {}""".format(library_name, ' '.join(make_read_group_file(library_name, rg, '.pruned.bam') for rg in sorted(library['readgroups'].keys()))), ioniced=True)

        printp("""\n# drmr:wait""")

        printp("""\n# drmr:job nodes=1 processors=1 memory=8g working_directory={} time_limit=4h""".format(MACS2_DIR))
        printp("""\n# drmr:label macs2-libraries\n""")
        for library_name, library in sorted(LIBRARIES.items()):
            input_bam = '{}.library.pruned.bam'.format(library_name)
            prefix = library_name
            symlink(os.path.join(PRUNE_DIR, input_bam), MACS2_DIR)
            printp(MACS2_BROAD_PEAK_TEMPLATE.format(**locals()), timed=True, ioniced=True)

    # If asked to call peaks on samples, make a pruned BAM for each,
    # and run MACS2 on those.
    if call_samples:
        printp("""# drmr:job nodes=1 processors=1 memory=4g time_limit=4h working_directory={}""".format(PRUNE_DIR))
        printp("""\n# drmr:label make-pruned-samples\n""")
        for index, (sample, libraries) in enumerate(sorted(SAMPLES.items()), 1):
            if LIMIT_IO and index % LIMIT_IO == 0:
                # limit the number of concurrent jobs to avoid thrashing the disk (set LIMIT_IO=False on clusters!)
                printp("""\n# drmr:wait""")

            pruned_readgroup_bams = []
            for library in sorted(libraries):
                pruned_readgroup_bams.extend(make_read_group_file(library['library'], rg, '.pruned.bam') for rg in sorted(library['readgroups'].keys()))

            printp("""\nsamtools merge -f {}.sample.pruned.bam {}""".format(sample, ' '.join(pruned_readgroup_bams)), ioniced=True)

        printp("""\n# drmr:wait""")

        printp("""\n# drmr:job nodes=1 processors=1 memory=8g working_directory={} time_limit=4h""".format(MACS2_DIR))
        printp("""\n# drmr:label macs2-samples\n""")
        for sample in sorted(SAMPLES.keys()):
            input_bam = '{}.sample.pruned.bam'.format(sample)
            prefix = sample
            symlink(os.path.join(PRUNE_DIR, input_bam), MACS2_DIR)
            printp(MACS2_BROAD_PEAK_TEMPLATE.format(**locals()), timed=True, ioniced=True)

    printp("""\n# drmr:wait""")

    printp("""\n#\n# Compress MACS2 BedGraph output\n#""")
    printp("""\n# drmr:label compress-macs2-output\n""")

    compression_targets = set()
    for sample, libraries in sorted(SAMPLES.items()):
        if call_samples:
            compression_targets.add(sample)
        for library in sorted(libraries):
            if call_libraries:
                compression_targets.add(library['library'])
            for rg in sorted(library['readgroups'].keys()):
                compression_targets.add(make_read_group_file(library['library'], rg))

    for target in sorted(compression_targets):
        compress_macs2_output(target)

    blacklist_targets = set()
    if exclude_blacklisted_regions:
        printp("""\n# drmr:wait""")
        printp("""\n# drmr:label exclude-blacklisted-peaks\n""")
        for sample, libraries in sorted(SAMPLES.items()):
            if call_samples:
                blacklist_targets.add((sample, libraries[0]['reference_genome']))

            for library in sorted(libraries):
                if call_libraries:
                    blacklist_targets.add((library['library'], library['reference_genome']))

                for rg in sorted(library['readgroups'].keys()):
                    prefix = make_read_group_file(library['library'], rg)
                    blacklist_targets.add((prefix, library['reference_genome']))


    for target, genome in sorted(blacklist_targets):
        exclude_blacklisted_peaks(target, genome)


def normalize_macs2_treatment_pileups():
    """Normalize MACS2 treatment pileup bedgraph files to 10 million fragments"""

    mkdir(GB_DIR)

    printp("""\n#\n# Normalize MACS2 treatment pileup bedgraph files\n#""")
    printp("""\n# drmr:label normalize-macs2-treatment-pileup""")
    printp("""\n# drmr:job time_limit=1h working_directory={} processors=1""".format(GB_DIR))

    template = """test -r {macs2_output_file} && (zcat {macs2_treat_pileup_file} | awk -v NTAGS=$(grep 'total tags in treatment' {macs2_output_file} | awk '{{print $NF}}') '{{$4=$4*(10000000/NTAGS); print}}' | perl -pe 's/\\s+/\\t/g; s/$/\\n/' | (grep -v '_' || true) | LC_COLLATE=C sort -k1,1 -k2,2n > {macs2_normalized_treat_pileup_file}) || exit 1"""

    NTAGS_RE = re.compile('total tags in treatment: (\d+)')
    for library_name, library in sorted(LIBRARIES.items()):
        for rg in sorted(library['readgroups'].keys()):
            prefix = make_read_group_file(library_name, rg)
            macs2_output_file = os.path.join(MACS2_DIR, '{}.macs2.out'.format(prefix))
            macs2_treat_pileup_file = os.path.join(MACS2_DIR, '{}.broad_treat_pileup.bdg.gz'.format(prefix))
            macs2_normalized_treat_pileup_file = os.path.join(GB_DIR, '{}.broad_treat_pileup.normalized.bdg'.format(prefix))
            printp(template.format(**locals()))

    printp("""\n# drmr:wait""")


def make_macs2_treatment_pileup_bigwigs():
    """Make bigwig files from the MACS2 treatment pileup bedgraph files"""

    mkdir(GB_DIR)

    printp("""\n#\n# Generate bigwig files from MACS2 treatment pileups\n#""")
    printp("""\n# drmr:label macs2-treatment-pileup-bigwig""")
    printp("""\n# drmr:job time_limit=1h working_directory={} processors=1""".format(GB_DIR))

    template = """bash -c 'test -s {macs2_normalized_treat_pileup_bdg} && bedGraphToBigWig {macs2_normalized_treat_pileup_bdg} {chromosome_sizes} {macs2_normalized_treat_pileup_bw} || true'"""

    for library_name, library in sorted(LIBRARIES.items()):
        chromosome_sizes = get_chromosome_sizes_path(library)
        if not os.path.isfile(chromosome_sizes):
            print('Chromosome size file "{}" does not exist.'.format(chromosome_sizes))
            sys.exit(1)

        for rg in sorted(library['readgroups'].keys()):
            prefix = make_read_group_file(library_name, rg)
            macs2_normalized_treat_pileup_bdg = os.path.join(GB_DIR, '{}.broad_treat_pileup.normalized.bdg'.format(prefix))
            macs2_normalized_treat_pileup_bw = os.path.join(GB_DIR, '{}.broad_treat_pileup.normalized.bw'.format(prefix))
            printp(template.format(**locals()), ioniced=True)

    printp("""\n# drmr:wait""")


def make_track_lines():
    """Make UCSC Genome Browser track lines for the broad peak files and the normalized treatment pileup bigwig files"""

    base_url = 'https://theparkerlab.med.umich.edu/gb/tracks'

    mkdir(GB_DIR)

    for reference_genome, libraries in libraries_by_genome():
        track_lines_file = os.path.join(GB_DIR, 'track_lines.' + reference_genome)
        with open(track_lines_file, 'w') as track_lines:
            for library in libraries:
                for rg in sorted(library['readgroups'].keys()):
                    prefix = make_read_group_file(library['library'], rg)
                    treat_pileup_bw = '{}.broad_treat_pileup.normalized.bw'.format(prefix)
                    treat_pileup_bw_url = '{}/{}/{}'.format(base_url, ANALYSIS_NAME, treat_pileup_bw)

                    treat_pileup_track_line = (
                        """track type=bigWig db={library[reference_genome]} name='{prefix}' """
                        """visibility=full color=255,128,0 alwaysZero=on maxHeightPixels=50:50:50 """
                        """windowingFunction=mean smoothingWindow=3 bigDataUrl={treat_pileup_bw_url}\n"""
                    ).format(**locals())

                    track_lines.write(treat_pileup_track_line)

                    peak_track = '{}.broad_peaks.broadPeak.gz'.format(prefix)
                    peak_track_url = '{}/{}/{}'.format(base_url, ANALYSIS_NAME, peak_track)

                    peak_track_line = (
                        """track type=broadPeak db={library[reference_genome]} name='{prefix}_peaks'\n"""
                        """{peak_track_url}\n"""
                    ).format(**locals())

                    track_lines.write(peak_track_line)

                    symlink(os.path.join(MACS2_DIR, peak_track), GB_DIR)

    with open(os.path.join(GB_DIR, 'install.sh'), 'w') as install:
        vs_dir = os.path.join('/lab/web/data/gb/tracks/', ANALYSIS_NAME)
        install.write("""ssh vs mkdir -p {} && rsync -avL --exclude install.sh {}/ vs:{}/ && ssh vs "chmod 755 {}; chmod 644 {}/*"\n""".format(vs_dir, GB_DIR, vs_dir, vs_dir, vs_dir))


def ataqv(use_autosomal_reference_files=False, threads=4):
    """
    Run ataqv for each library.
    """

    mkdir(ATAQV_DIR)

    printp("""\n#\n# run ataqv\n#""")
    printp("""\n# drmr:label ataqv""")

    ATAQV_TEMPLATE = """ataqv --peak-file {peak_file} --metrics-file {metrics_file} --name 'Library {library_name}, read group {rg}' {ataqv_options} {organism} {alignments} > {output_file}\n"""

    metrics_files = []
    for library_name, library in sorted(LIBRARIES.items()):
        if use_autosomal_reference_files:
            autosomal_reference_filename = os.path.join(ATAQV_DIR, '{}.arf'.format(library_name))
            with open(autosomal_reference_filename, 'w') as arf:
                for ar in sorted(AUTOSOMAL_REFERENCES[library['reference_genome']]):
                    arf.write(ar)
                    arf.write('\n')

        for rg in sorted(library['readgroups'].keys()):
            alignments = make_read_group_file(library_name, rg, '.md.bam')
            alignments_index = alignments + '.bai'
            peak_file = make_read_group_file(library_name, rg, '.broad_peaks.broadPeak.gz')
            symlink(os.path.join(MD_DIR, alignments), ATAQV_DIR)
            symlink(os.path.join(MD_DIR, alignments_index), ATAQV_DIR)
            symlink(os.path.join(MACS2_DIR, peak_file), ATAQV_DIR)

            ataqv_options = ["""{} '{}'""".format(k, v) for k, v in sorted({
                '--description': DESCRIPTION,
                '--library-description': library.get('description').replace('\n', ' '),
                '--url': library.get('url'),
            }.items()) if v]

            if use_autosomal_reference_files:
                ataqv_options.append('--autosomal-reference-file {}'.format(autosomal_reference_filename))

            tss_file = TSS_REFERENCES.get(library['reference_genome'])
            if tss_file:
                if os.path.exists(tss_file):
                    ataqv_options.append('--tss-file {}'.format(tss_file))
                else:
                    print('TSS reference "{}" for genome "{}" not found.'.format(tss_file, library['reference_genome']), file=sys.stderr)
                    sys.exit(1)

            if library['reference_genome'] in EXCLUDED_REGIONS:
                for excluded_region_file in sorted(EXCLUDED_REGIONS[library['reference_genome']]):
                    if os.path.isfile(excluded_region_file):
                        ataqv_options.append('--excluded-region-file {}'.format(excluded_region_file))
                    else:
                        print('Excluded region file {} for reference genome {} does not exist; dropping it from ataqv commands.'.format(excluded_region_file, library['reference_genome']), file=sys.stderr)

            ataqv_options = ' '.join(ataqv_options)

            organism = ORGANISMS.get(library['reference_genome'])

            metrics_file = make_read_group_file(library_name, rg, '.ataqv.json.gz')
            metrics_files.append(metrics_file)

            output_file = make_read_group_file(library_name, rg, '.ataqv.out')

            printp("""\n# drmr:job nodes=1 processors={} memory=8g working_directory={} time_limit=4h""".format(threads, ATAQV_DIR))
            printp(ATAQV_TEMPLATE.format(**locals()), timed=True, ioniced=True)

    printp("""\n# drmr:wait""")

    printp("""\n# drmr:label mkarv""")
    printp("""\n# drmr:job nodes=1 processors=1 working_directory={} time_limit=4h""".format(ATAQV_DIR))
    printp("""\nmkarv --force -d '{}' viewer {}""".format(DESCRIPTION, ' '.join(metrics_files)))


if __name__ == '__main__':
    mkdir(WORK_PATH)
    mkdir(DATA_PATH)

    for source_file in iterate_all_source_files():
        dest = os.path.join(DATA_PATH, os.path.basename(source_file))
        symlink(source_file, dest, absolute=True)

    if os.path.exists(PIPELINE):
        os.unlink(PIPELINE)

    PIPELINE_FILE = open(PIPELINE, 'w')
    printp = functools.partial(print_to_pipeline, PIPELINE_FILE)

    printp("""#!/bin/bash""")
    printp("""# -*- mode: sh; coding: utf-8 -*-\n""")

    fastqc()

    trim_adapters()

    printp("""\n# drmr:wait""")

    bwa()

    printp("""\n# drmr:wait""")

    mark_duplicates()

    printp("""\n# drmr:wait""")

    prune()

    printp("""\n# drmr:wait""")

    macs2(exclude_blacklisted_regions=True)

    printp("""\n# drmr:wait""")

    normalize_macs2_treatment_pileups()

    make_macs2_treatment_pileup_bigwigs()

    make_track_lines()

    ataqv()